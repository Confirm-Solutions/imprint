\documentclass[10pt, a4paper]{article}
\usepackage[english]{babel}
\usepackage{fullpage}
\input{../../../latex/mathtools}

\begin{document}
\title{Generalized Grid Upper Bound}
\author{James Yang}
\maketitle

\section*{Introduction}

The current formulation of upper bound assumes that the 
(rectangular) gridding occurs in the (canonical) natural parameter space.
However, it is sometimes more suitable to grid 
a differently parametrized space.
For example, the Exponential control k treatment model
works better under the parametrization of $\lambda_{c}, h$
where $\lambda_c$ is the natural parameter of the control arm
and $h$ is the hazard rate.
Moreover, for better scaling, we may want to grid 
rather the $\log(\lambda_{c}), \log(h)$.
Such parametrization defines a mapping from 
\emph{the grid space} to the natural parameter space.
We wish to construct the upper bound under any such parametrization,
provided that the mapping is sufficiently smooth.

In the subsequent sections,
we will use the notation $\theta \in \R^p$ to denote 
a point in the grid-space and $\eta = \eta(\theta) \in \R^d$
as the canonical natural parameter.
Assume that $\eta$ is twice-continuously differentiable.

\section*{Upper Bound Changes}

The 0th order Monte Carlo term and its upper bound
need no change from reparametrization.
As in Michael's thesis, we will denote $f(\theta) = P_\theta(A)$
where $A$ is the event of false rejection.

\subsection*{Gradient Term: $\delta_1$}

\begin{align*}
    \nabla f(\theta)
    :=
    \nabla_{\theta} P_\theta(A) 
    &=
    \nabla_{\theta} \int_A \frac{P_{\theta}}{P_{\theta_0}} dP_{\theta_0}
    =
    \int_A \nabla_{\theta} \frac{P_{\theta}}{P_{\theta_0}} dP_{\theta_0}
    \\&=
    \int_A (D_{\theta}\eta)^\top 
    \nabla_{\eta} \frac{P_{\eta}}{P_{\eta_0}} dP_{\eta_0}
\end{align*}
where $\eta_0 = \eta(\theta_0)$.
If $\theta_0$ is the point at which we are Taylor expanding,
it suffices to compute this gradient at $\theta = \theta_0$.
This results in
\begin{align*}
    \nabla_{\theta} P_{\theta_0}(A) 
    &=
    \int_A (D_{\theta}\eta(\theta_0))^\top (T - \nabla_\eta A(\eta_0)) dP_{\theta_0}
\end{align*}

Hence, our gradient Monte Carlo estimate will be
\begin{align*}
    \hat{\nabla f}(\theta_0)
    :=
    D_{\theta} \eta(\theta_0)^\top 
    \frac{1}{N}
    \sum\limits_{i=1}^N
    (T(X_i)-\nabla_\eta A(\eta_0)) \indic{X_i \in A}
\end{align*}

Note that the Jacobian is known when defining a model
and is simulation-independent.
Further, it only changes how we compute the upper bound
and does not affect the InterSum updates (updating the gradient array).

\subsection*{Gradient Upper Bound Term: $\delta_1^u$}

We follow a similar progression as in the original method in Michael's thesis.
Once we can show for any corner difference $v_m$, 
there exists a corresponding random $c_m$ such that
\[
    P_\theta\paren{v_m^\top \nabla f(\theta) \leq c_m} \geq 1-\delta
\]
then we have
\[
    P_\theta\paren{\sup\limits_{v\in R_0} v^\top \nabla f(\theta) \leq \max\limits_{m} c_m} \geq 1-\delta
\]

Using Cantelli's inequality
with $Y = v_m^\top \hat{\nabla f(\theta)} = \frac{1}{N} \sum\limits_{i=1}^N v_m^\top \hat{\nabla f(\theta)}_i$,
we just need to provide an upper bound on the variance of $v_m^\top \hat{\nabla f(\theta)}_i$,
where $\hat{\nabla f(\theta)}_i := D_\theta \eta(\theta)^\top (T(X_i)-\nabla_\eta A(\eta))$.
In that endeavor,
\begin{align*}
    \var{v_m^\top \hat{\nabla f(\theta)}_i}
    &=
    v_m^\top \var{\hat{\nabla f(\theta)}_i} v_m
    \leq 
    v_m^\top (D_\theta \eta)^\top \var{T_{\tau_{max}}} (D_\theta \eta) v_m
\end{align*}
The rest of the calculations remain the same.

Hence, our 1st order upper bound term is simply
\begin{align*}
    \hat{\delta}_1^u
    &=
    \sqrt{
        \frac{v_m^\top (D_\theta \eta)^\top \var{T_{\tau_{max}}} (D_\theta \eta) v_m}{N}
        \paren{\frac{1}{\delta} - 1}
    }
\end{align*}

\subsection*{Hessian Term}

For any corner difference $v$,
we will first upper bound
\begin{align*}
    \int_0^1 (1-\alpha) v^\top \nabla^2 f(\theta_0 + \alpha v) v d\alpha
\end{align*}

It suffices to bound $\nabla^2 f(\theta)$.
\begin{align*}
    \nabla^2 f(\theta)
    &=
    \int_A \nabla^2 P_\theta(x) dx
\end{align*}
Applying the multivariate chain-rule for the function
$\theta \mapsto P_{\eta(\theta)}(x)$,
we have that
\begin{align*}
    \nabla^2 P_\theta(x)
    &=
    (D\eta)^\top \nabla^2 P_\eta(x) (D\eta)
    + 
    \sum\limits_{k=1}^d 
    \frac{\partial P_\eta}{\partial \eta_k}
    \nabla^2 \eta_k
\end{align*}
~\cite{skorski:2019:hess}.

By the same proof as in the thesis,
\begin{align*}
    -\var{T_{\tau_{\max}}}_{\eta}
    \preceq
    \int_A \nabla^2 P_{\eta}(x) dx
    \preceq
    \var{T_{\tau_{\max}}}_{\eta}
\end{align*}

Note that if $S \preceq T$
for any square matrices $S, T$,
then we must have that for any matrix $A$,
$A^\top S A \preceq A^\top T A$.
This is because $S \preceq T$
if and only if $T - S$ is positive semi-definite,
and $A^\top (T-S) A$ is clearly positive semi-definite as well.
Rearranging, we have our claim.
Hence, 
\begin{align*}
    -(D\eta)^\top
    \var{T_{\tau_{\max}}}_{\eta}
    (D\eta)
    \preceq
    (D\eta)^\top
    \int_A \nabla^2 P_{\eta}(x) dx
    (D\eta)
    \preceq
    (D\eta)^\top
    \var{T_{\tau_{\max}}}_{\eta}
    (D\eta)
\end{align*}

This gives us the first bound:
\begin{align}
    v^\top \nabla^2 f(\theta) v
    \leq
    v^\top (D\eta)^\top \var{T_{\tau_{\max}}}_{\eta} (D\eta) v
    +
    \sum\limits_{k=1}^d 
    v^\top \nabla^2 \eta_k v
    \int_A (T(x) - \nabla A(\eta))_k P_\eta(x) dx
    \label{eq:hess-second-term}
\end{align}

We next bound second term in Eq.~\ref{eq:hess-second-term}.
\begin{align*}
    \int_A \abs{(T(x) - \nabla A(\eta))_k} P_\eta(x) dx
    &\leq
    \int \abs{(T(x) - \nabla A(\eta))_k} P_\eta(x) dx
    \\&\leq
    \paren{\int \abs{(T(x) - \nabla A(\eta))_k}^2 P_\eta(x) dx}^{1/2}
    \\&=
    \sqrt{\var{T_k}_\eta}
\end{align*}

Combining with Eq.~\ref{eq:hess-second-term},
\begin{align*}
    v^\top \sup\limits_{\theta \in R} \nabla^2 f(\theta) v
    &\leq
    v^\top \sup\limits_{\theta \in R} \bracket{(D\eta(\theta))^\top \var{T_{\tau_{\max}}}_{\eta} (D\eta)} v
    +
    \sum\limits_{k=1}^d 
    \sup\limits_{\theta \in R}
    \bracket{%
        \abs{v^\top \nabla^2 \eta_k(\theta) v}
        \sqrt{\var{T_k}_{\eta(\theta)}}
    }
    \\&\leq
    v^\top \sup\limits_{\theta \in R} \bracket{(D\eta(\theta))^\top \var{T_{\tau_{\max}}}_{\eta} (D\eta)} v
    +
    \norm{v}^2
    \sum\limits_{k=1}^d 
    \sup\limits_{\theta \in R}
    \bracket{%
        \norm{\nabla^2 \eta_k(\theta)}_{op}
        \sqrt{\var{T_k}_{\eta(\theta)}}
    }
\end{align*}

This gives us our upper bound for the hessian term:
\begin{align*}
    \hat{\delta}_2^u
    &=
    \frac{1}{2}
    \paren{%
        v_m^\top \sup\limits_{\theta \in R} \bracket{(D\eta(\theta))^\top \var{T_{\tau_{\max}}}_{\eta} (D\eta)} v_m
        +
        \norm{v_m}^2
        \sum\limits_{k=1}^d 
        \sup\limits_{\theta \in R}
        \bracket{%
            \norm{\nabla^2 \eta_k(\theta)}_{op}
            \sqrt{\var{T_k}_{\eta(\theta)}}
        }
    }
\end{align*}
for some corner vector $v_m$.

Note that the second term becomes 0 for any linear transformation $\eta$.
In particular, for the identity transformation, it simplifies to the usual formula
\begin{align*}
    \frac{1}{2}
    v_m^\top \sup\limits_{\theta \in R} \var{T_{\tau_{\max}}}_\eta v_m
\end{align*}

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
