{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import inla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_gauss_rule(N, a, b):\n",
    "    A = np.log(a)\n",
    "    B = np.log(b)\n",
    "    p, w = inla.gauss_rule(N, a=A, b=B)\n",
    "    pexp = np.exp(p)\n",
    "    wexp = np.exp(p) * w\n",
    "    return (pexp, wexp)\n",
    "\n",
    "a = 1e-8\n",
    "b = 1e3\n",
    "pexp, wexp = log_gauss_rule(90, a, b)\n",
    "alpha = 0.0005\n",
    "beta = 0.000005\n",
    "f = scipy.stats.invgamma.pdf(pexp, alpha, scale=beta)\n",
    "exact = scipy.stats.invgamma.cdf(b, alpha, scale=beta) - scipy.stats.invgamma.cdf(a, alpha, scale=beta)\n",
    "est = np.sum(f * wexp)\n",
    "plt.plot(np.log(pexp) / np.log(10), f)\n",
    "plt.xlabel('$log_{10}\\sigma^2$')\n",
    "plt.ylabel('$PDF$')\n",
    "plt.show()\n",
    "print('exact CDF: ', exact),\n",
    "print('numerical integration CDF: ', est)\n",
    "print('error: ', est - exact)\n",
    "sigma2_rule = (pexp, wexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model and simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match with the INLA notation, the $\\theta$ parameter for Berry et al 2013 is called $x$ here. Also, Berry uses:\n",
    "\\begin{equation}\n",
    "\\tilde{x} = logit(p) - logit(p_1)\n",
    "\\end{equation}\n",
    "whereas we use:\n",
    "\\begin{equation}\n",
    "x = logit(p) = \\tilde{x} + logit(p_1)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.full(4, 0.1) # rate of response below this is the null hypothesis\n",
    "p1 = np.full(4, 0.3) # rate of response above this is the alternative hypothesis.\n",
    "# rate of response > p0 and < p1 is gray area.\n",
    "p0_x = scipy.special.logit(p0)\n",
    "\n",
    "pmid = (p0 + p1) / 2\n",
    "pmid_x = scipy.special.logit(pmid)\n",
    "\n",
    " # final evaluation criterion \n",
    " # accept the alternative hypo if Pr(p[i] > p0|data) > pfinal_thresh[i]\n",
    "pfinal_thresh = np.full(4, 0.85)\n",
    "\n",
    "# early stopping criterion\n",
    "pmid_accept = 0.9\n",
    "pmid_fail = 0.05\n",
    "\n",
    "null_x_berry = np.log(p0 / (1 - p0)) - np.log(p1 / (1 - p1))\n",
    "null_x = np.log(p0 / (1 - p0))\n",
    "prior_mu_mean = null_x[0]\n",
    "\n",
    "def berry_prior(theta):\n",
    "    mu = theta[..., 0]\n",
    "    mu_prior = scipy.stats.norm.logpdf(mu, prior_mu_mean, 100)\n",
    "\n",
    "    sigma2 = theta[..., 1]\n",
    "    alpha = 0.0005\n",
    "    beta = 0.000005\n",
    "    sigma2_prior = scipy.stats.invgamma.logpdf(sigma2, alpha, scale=beta)\n",
    "    return mu_prior + sigma2_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got this data by deconstructing the graphs in in Figure 1 of Berry et al 2013.\n",
    "n_i = np.array([[i] * 4 for i in [10,15,20,25,30,35]])\n",
    "y_i = np.array([[1, 6, 3, 3], [3, 8, 5, 4], [6,9,7,5], [7,10,8,7], [8,10,9,8], [11, 11, 10, 9]])\n",
    "data = np.stack((y_i, n_i), axis=2)\n",
    "\n",
    "model = inla.binomial_hierarchical()\n",
    "model.log_prior = berry_prior\n",
    "\n",
    "mu_rule = inla.gauss_rule(201, -5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirty Bayes for the Berry model\n",
    "\n",
    "## Notation\n",
    "* $d$ is the number of arms in the trial\n",
    "* $\\hat{I}$ is the sample information matrix.\n",
    "* $\\mathbf{y}$ is the *vector* probability-space data.\n",
    "* $\\mathbf{n}$ is the *vector* number of patients per arm.\n",
    "* $\\mathbf{\\theta}$ is the *vector* logit-space true parameters\n",
    "* $\\hat{\\mathbf{\\theta}}$ is the *vector* logit-space observed values\n",
    "* $\\mu$ is the mean of the sharing distribution\n",
    "* $\\sigma$ is the standard deviation of the sharing distribution\n",
    "* $\\mu_0 = -1.34$ is the mean of the $\\mu$ prior.\n",
    "* $S = 10$ is the std dev of the $\\mu$ prior.\n",
    "\n",
    "## The basic model\n",
    "\\begin{align}\n",
    "\\mathbf{y}\\mathbf{n} &\\sim Binomial( \\mathbf{\\theta}, \\mathbf{n})\\\\\n",
    "\\mathbf{\\theta} &\\sim N(\\mu, \\sigma^2)\\\\\n",
    "\\mu &\\sim N(\\mu_0, S^2)\\\\\n",
    "\\sigma^2 &\\sim InvGamma(0.0005, 0.000005)\\\\\n",
    "\n",
    "P(y, \\theta, \\sigma^2) &= P(\\sigma^2) P(\\theta|\\sigma^2) P(y|\\theta)\n",
    "\\end{align}\n",
    "\n",
    "## Section 1: Approximating the posterior conditional on hyperparameters\n",
    "\n",
    "### Step #1, exact, integrate out $\\mu$, just use normal distribution conjugacy\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu) &= N(\\mu_0, S^2)\\\\\n",
    "P(\\theta|\\mu, \\sigma^2) &= N(\\mu_0, \\sigma^2)\\\\\n",
    "P(\\theta|\\sigma^2) &= P(\\theta|\\mu_0, \\sigma^2, S^2) =  N(\\mu_0, \\sigma^2 I + S^2 J)\n",
    "\\end{align}\n",
    "\n",
    "where J is the matrix of all 1s (everywhere, not just the diagonal?).\n",
    "\n",
    "### Step #2, the important approximation!! Binomial asymptotically is Normal \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{y} \\sim N(\\mathbf{\\theta}, I^{-1})\n",
    "\\end{align}\n",
    "\n",
    "### Step #3, introduce data to approximate likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "I^{-1} \\approx \\hat{I}^{-1} = \\hat{p} (1 - \\hat{p}) / n\\\\\n",
    "P(y|\\theta) = N(\\mathbf{\\theta}, I^{-1}) \\approx N(\\hat{\\mathbf{\\theta}}, \\hat{I}^{-1})\\\\\n",
    "\\end{align}\n",
    "\n",
    "### Step #4, combine the two Gaussians $P(\\theta|y)$ and $P(\\theta|\\sigma^2)$:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\theta|y,\\sigma^2) = \\frac{P(\\theta|\\sigma^2)P(y|\\theta)}{P(y|\\sigma^2)}\n",
    "\\end{align}\n",
    "\n",
    "Using Gaussian posterior conjugacy: \n",
    "\\begin{align}\n",
    "\\theta|y, \\sigma^2 \\sim N(\\mu_d, \\Sigma_d)\\\\\n",
    "\\mu_d = ...\\\\\n",
    "\\Sigma_d = ...\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Integration over hyperparameters\n",
    "### Step #5: integrate out $\\sigma^2$\n",
    "\n",
    "\\begin{align}\n",
    "P(\\theta|y) = \\int P(\\theta| y, \\sigma^2)P(\\sigma^2|y) d\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "### Step #6: manipulate $\\sigma^2$ posterior\n",
    "\n",
    "By definition of conditional prob:\n",
    "\\begin{align}\n",
    "P(\\sigma^2|y) = \\frac{P(\\sigma^2, y)}{P(y)}\n",
    "\\end{align}\n",
    "\n",
    "### Step #7: computing the joint distribution $P(\\sigma^2, y)$\n",
    "\n",
    "\\begin{align}\n",
    "P(\\sigma^2, y) &= P(\\sigma^2)P(y|\\sigma^2)\\\\\n",
    "y|\\sigma^2 &\\sim N(\\mu, \\hat{I}^{-1} + \\Sigma_{\\sigma^2})\n",
    "\\end{align}\n",
    "Also, recall the prior:\n",
    "\\begin{align}\n",
    "\\sigma^2 &\\sim InvGamma(0.0005, 0.000005)\\\\\n",
    "\\end{align}\n",
    "\n",
    "### Step #8: compute $P(y)$ posterior\n",
    "\n",
    "Once we have computed the joint probability $P(\\sigma^2, y)$, the denominator/marginal/data-prior $P(y)$ can be easily computed by numerically integrating over $\\sigma^2$.\n",
    "\n",
    "### Step #9: actually do the integral from step #5.\n",
    "\n",
    "Because this integral is done numerically, we now have a mixture of Gaussians. The mean and variance of a weighted mixture of gaussians is a well-known formula: INSERT FORMULA HERE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_invert(S, d):\n",
    "    for k in range(len(d)):\n",
    "        offset = (d[k] / (1 + d[k] * S[k, k])) * np.outer(\n",
    "            S[k],\n",
    "            S[..., k],\n",
    "        )  # I wonder how to cheaply represent this outer? but in C++ it should be just a trivial for-loop anyway\n",
    "        S = S - offset\n",
    "    return S\n",
    "\n",
    "def calc_posterior_x(\n",
    "    sigma_sq: float,\n",
    "    mu_sig_sq: float,\n",
    "    sample_I,\n",
    "    thetahat,\n",
    "    mu_0,\n",
    "    d\n",
    "):\n",
    "    assert len(sigma_sq) == 1, sigma_sq\n",
    "    S_0 = np.diag(np.repeat(sigma_sq, d)) + mu_sig_sq\n",
    "    # V_0 = solve(S_0) #but because this is a known case of the form aI + bJ, we can use the explicit\n",
    "    # inverse formula, given by: 1/a I - J*(b/(a(a+db)))\n",
    "    V_0 = np.diag(np.repeat(1 / sigma_sq, d)) - (mu_sig_sq / sigma_sq) / (\n",
    "        sigma_sq + d * mu_sig_sq\n",
    "    )\n",
    "    Sigma_posterior = fast_invert(S_0, sample_I)\n",
    "    # precision_posterior = V_0 + np.diag(sample_I)\n",
    "    # Sigma_posterior = np.linalg.inv(precision_posterior)\n",
    "    mu_posterior = Sigma_posterior @ (sample_I * thetahat + V_0 @ mu_0)\n",
    "    return mu_posterior, np.diag(Sigma_posterior)\n",
    "\n",
    "def calc_dirty_bayes(y_i, n_i):\n",
    "    N = 6\n",
    "    d = 4\n",
    "    phat = y_i[:, :] / n_i[:, :]\n",
    "    thetahat = scipy.special.logit(phat)\n",
    "    sample_I = n_i[:, :] * phat * (1 - phat)  # diag(n*phat*(1-phat))\n",
    "    mu_0 = np.full_like(phat, -1.34)\n",
    "    mu_sig_sq = 100\n",
    "\n",
    "    mu_posterior = np.empty((N, sigma2_rule[0].shape[0], d))\n",
    "    sigma2_posterior = np.empty((N, sigma2_rule[0].shape[0], d))\n",
    "    joint_sigma2_y = np.empty((N, sigma2_rule[0].shape[0], d))\n",
    "    for i in range(N):\n",
    "        for j in range(sigma2_rule[0].shape[0]):\n",
    "            # Step 1-4: see above.\n",
    "            sig2 = sigma2_rule[0][j]\n",
    "            (\n",
    "                mu_posterior[i, j],\n",
    "                sigma2_posterior[i, j],\n",
    "            ) = calc_posterior_x(\n",
    "                np.array([sig2]),\n",
    "                np.array([mu_sig_sq]),\n",
    "                sample_I[i],\n",
    "                thetahat[i],\n",
    "                mu_0[i],\n",
    "                d,\n",
    "            )\n",
    "            # Step 6/7\n",
    "            prior = scipy.stats.invgamma.pdf(sig2, 0.0005, scale=0.000005)\n",
    "            y_given_sig2 = scipy.stats.multivariate_normal.pdf(\n",
    "                thetahat[i],\n",
    "                mu_0[i],\n",
    "                (\n",
    "                    np.diag(sample_I[i] ** -1)\n",
    "                    + np.diag(np.repeat(sig2, d))\n",
    "                    + mu_sig_sq\n",
    "                ),\n",
    "            )\n",
    "            joint_sigma2_y[i, j] = prior * y_given_sig2\n",
    "\n",
    "    # Step 8\n",
    "    py = np.sum(joint_sigma2_y * sigma2_rule[1][None, :, None], axis=1)\n",
    "    sigma2_given_y = joint_sigma2_y / py[:, None, :]\n",
    "\n",
    "    # Step 9: \n",
    "    weights = sigma2_given_y * sigma2_rule[1][None, :, None]\n",
    "    mu_db = np.sum(mu_posterior * weights, axis=1)\n",
    "    T = (mu_posterior - mu_db[:, None, :]) ** 2 + sigma2_posterior\n",
    "    sigma2_db = np.sum(T * weights, axis=1)\n",
    "    sigma_db = np.sqrt(sigma2_db)\n",
    "    return mu_db, sigma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_theta, report = inla.calc_posterior_theta(model, data, (mu_rule, sigma2_rule))\n",
    "mu_inla, sigma_inla = inla.calc_posterior_x(post_theta, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_db, sigma_db = calc_dirty_bayes(y_i, n_i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1.0 = \\sum_i f(x_i) w_i$$\n",
    "\n",
    "\"CDF\" is:\n",
    "* $f(x_i)$\n",
    "\n",
    "\"cumulative prob\" is:\n",
    "* $f(x_i) * w_i$\n",
    "\n",
    "the cdf would be... \n",
    "$$ F(x_j) = \\sum_{i < j} f(x_i) w_i$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure1_plot(title, data, mu_post, sigma_post):\n",
    "    # expit(mu_post) is the posterior estimate of the mean probability.\n",
    "    p_post = scipy.special.expit(mu_post)\n",
    "\n",
    "    # two sigma confidence intervals transformed from logit to probability space.\n",
    "    cilow = scipy.special.expit(mu_post - 2 * sigma_post)\n",
    "    cihigh = scipy.special.expit(mu_post + 2 * sigma_post)\n",
    "\n",
    "    y = data[:, :, 0]\n",
    "    n = data[:, :, 1]\n",
    "\n",
    "    # The simple ratio of success to samples. Binomial \"p\".\n",
    "    raw_ratio = y / n\n",
    "    p_success = np.empty_like(mu_post)\n",
    "\n",
    "    # early stopping criterion\n",
    "    p_success[:5] = 1.0 - scipy.stats.norm.cdf(pmid_x, mu_post[:5], sigma_post[:5])\n",
    "    # final success criterion\n",
    "    p_success[5] = 1.0 - scipy.stats.norm.cdf(p0_x, mu_post[5], sigma_post[5])\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(title)\n",
    "    outergs = fig.add_gridspec(2, 3, hspace=0.3)\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        innergs = outergs[i].subgridspec(\n",
    "            2, 1, wspace=0, hspace=0, height_ratios=[0.7, 0.3]\n",
    "        )\n",
    "\n",
    "        plt.subplot(innergs[0])\n",
    "        plt.plot(np.arange(4), raw_ratio[i], \"kx\")\n",
    "        plt.plot(np.arange(4), p_post[i], \"ko\", mfc=\"none\")\n",
    "        plt.plot(np.arange(4), p_success[i], \"k \", marker=(8, 2, 0))\n",
    "\n",
    "        plt.vlines(np.arange(4), cilow[i], cihigh[i], color=\"k\", linewidth=1.0)\n",
    "\n",
    "        if i < 5:\n",
    "            plt.title(f\"Interim Analysis {i+1}\")\n",
    "            plt.hlines([pmid_fail, pmid_accept], -1, 4, colors=[\"k\"], linestyles=[\"--\"])\n",
    "            plt.text(-0.1, 0.91, \"Early Success\", fontsize=7)\n",
    "            plt.text(2.4, 0.06, \"Early Futility\", fontsize=7)\n",
    "        else:\n",
    "            plt.title(\"Final Analysis\")\n",
    "            plt.hlines([pfinal_thresh[0]], -1, 4, colors=[\"k\"], linestyles=[\"--\"])\n",
    "            plt.text(-0.1, 0.86, \"Final Success\", fontsize=7)\n",
    "\n",
    "        plt.xlim([-0.3, 3.3])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.yticks(np.linspace(0.0, 1.0, 6))\n",
    "        plt.xlabel(\"Group\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "\n",
    "        plt.subplot(innergs[1])\n",
    "        plt.bar(\n",
    "            [0, 1, 2, 3],\n",
    "            n[i],\n",
    "            tick_label=[str(i) for i in range(4)],\n",
    "            color=(0.6, 0.6, 0.6, 1.0),\n",
    "            edgecolor=\"k\",\n",
    "            zorder=0,\n",
    "        )\n",
    "        plt.bar(\n",
    "            [0, 1, 2, 3],\n",
    "            y[i],\n",
    "            color=(0.6, 0.6, 0.6, 1.0),\n",
    "            hatch=\"////\",\n",
    "            edgecolor=\"w\",\n",
    "            lw=1.0,\n",
    "            zorder=1,\n",
    "        )\n",
    "        #         # draw hatch\n",
    "        # ax1.bar(range(1, 5), range(1, 5), color='none', edgecolor='red', hatch=\"/\", lw=1., zorder = 0)\n",
    "        # # draw edge\n",
    "        plt.bar([0, 1, 2, 3], y[i], color=\"none\", edgecolor=\"k\", zorder=2)\n",
    "        ticks = np.arange(0, 36, 5)\n",
    "        plt.yticks(ticks, [str(i) if i % 10 == 0 else \"\" for i in ticks])\n",
    "        plt.xticks(np.arange(4), ['1', '2', '3', '4'])\n",
    "        plt.gca().yaxis.set_label_position(\"right\")\n",
    "        plt.gca().yaxis.tick_right()\n",
    "        plt.xlabel(\"Group\")\n",
    "        plt.ylabel(\"N\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_plot('INLA', data, mu_inla, sigma_inla)\n",
    "figure1_plot('DB', data, mu_db, sigma_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I got this data by deconstructing the graphs in in Figure 1 of Berry et al 2013.\n",
    "n_i2 = np.array([[10, 10, 10, 10], [15, 15, 15, 15], [20, 20, 20, 20], [20, 20, 25, 25], [20, 20, 30, 30], [20, 20, 35, 35]])\n",
    "y_i2 = np.array([[0, 1, 3, 3], [0, 1, 4, 5], [0, 1, 6, 6], [0, 1, 6, 7], [0, 1, 7, 8], [0, 1, 9, 10]], dtype=np.float64)\n",
    "data2 = np.stack((y_i2, n_i2), axis=2)\n",
    "mu_rule = inla.gauss_rule(201, -7, 1.5)\n",
    "\n",
    "model = inla.binomial_hierarchical()\n",
    "model.log_prior = berry_prior\n",
    "post_theta2, report2 = inla.calc_posterior_theta(model, data2, (mu_rule, sigma2_rule))\n",
    "mu_inla2, sigma_inla2 = inla.calc_posterior_x(post_theta2, report2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_i2_db = y_i2.copy()\n",
    "y_i2_db[y_i2==0] += 1e-1\n",
    "mu_db2, sigma_db2 = calc_dirty_bayes(y_i2_db, n_i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_plot('INLA', data2, mu_inla2, sigma_inla2)\n",
    "figure1_plot('DB', data2, mu_db2, sigma_db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness in the marginals\n",
    "\n",
    "Why is the confidence interval on the 0-th arm in the figure above so large? This is a case where one of the core INLA assumptions breaks down. INLA assumes that p(x|y,\\theta) is approximately normal. In this particular case, that assumption is not correct. Intuitively, with 0 successes out of 20 patients, there is a lot more potential for small $x_0$ values than potential for large $x_0$ values. As you can see below, there is substantial skewness. There are approaches to deal with this. See here: https://github.com/mikesklar/kevlar/issues/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_vs = np.linspace(-15, 5, 100)\n",
    "x123_vs = np.full_like(x0_vs, -1.0)\n",
    "x = np.array([x0_vs, x123_vs, x123_vs, x123_vs]).T.copy()\n",
    "lj = model.log_joint(model, x, data[0], np.array([[-1.0, 10.0]]))\n",
    "plt.plot(x0_vs, np.exp(lj))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_post, sigma_post = inla.calc_posterior_x(post_theta2, report2)\n",
    "\n",
    "# expit(mu_post) is the posterior estimate of the mean probability.\n",
    "p_post = scipy.special.expit(mu_post)\n",
    "\n",
    "# two sigma confidence intervals transformed from logit to probability space.\n",
    "cilow = scipy.special.expit(mu_post - 2 * sigma_post)\n",
    "cihigh = scipy.special.expit(mu_post + 2 * sigma_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cilow[0], cihigh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum = np.sum(np.exp(lj))\n",
    "mean = x0_vs[np.argmax(np.exp(lj))]\n",
    "ci025 = x0_vs[np.argmax(np.cumsum(np.exp(lj)) / total_sum > 0.05)]\n",
    "ci975 = x0_vs[np.argmax(np.cumsum(np.exp(lj)) / total_sum > 0.95)]\n",
    "ci025, ci975, np.abs(mean-ci025), np.abs(mean-ci975), scipy.special.expit(ci025), scipy.special.expit(ci975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import inla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_gauss_rule(N, a, b):\n",
    "    A = np.log(a)\n",
    "    B = np.log(b)\n",
    "    p, w = inla.gauss_rule(N, a=A, b=B)\n",
    "    pexp = np.exp(p)\n",
    "    wexp = np.exp(p) * w\n",
    "    return (pexp, wexp)\n",
    "\n",
    "a = 1e-8\n",
    "b = 1e3\n",
    "pexp, wexp = log_gauss_rule(90, a, b)\n",
    "alpha = 0.0005\n",
    "beta = 0.000005\n",
    "f = scipy.stats.invgamma.pdf(pexp, alpha, scale=beta)\n",
    "exact = scipy.stats.invgamma.cdf(b, alpha, scale=beta) - scipy.stats.invgamma.cdf(a, alpha, scale=beta)\n",
    "est = np.sum(f * wexp)\n",
    "plt.plot(np.log(pexp) / np.log(10), f)\n",
    "plt.xlabel('$log_{10}\\sigma^2$')\n",
    "plt.ylabel('$PDF$')\n",
    "plt.show()\n",
    "print('exact CDF: ', exact),\n",
    "print('numerical integration CDF: ', est)\n",
    "print('error: ', est - exact)\n",
    "sigma2_rule = (pexp, wexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.array([0.05, 0.05, 0.1, 0.2]) # rate of response below this is the null hypothesis\n",
    "p1 = np.array([0.2, 0.2, 0.3, 0.4]) # rate of response above this is the alternative hypothesis.\n",
    "# p0 = np.array([0.1, 0.1, 0.1, 0.1]) # rate of response below this is the null hypothesis\n",
    "# p1 = np.array([0.3, 0.3, 0.3, 0.3]) # rate of response above this is the alternative hypothesis.\n",
    "# rate of response > p0 and < p1 is gray area.\n",
    "p0_x = scipy.special.logit(p0)\n",
    "\n",
    "pmid = (p0 + p1) / 2\n",
    "pmid_x = scipy.special.logit(pmid)\n",
    "\n",
    " # final evaluation criterion \n",
    " # accept the alternative hypo if Pr(p[i] > p0|data) > pfinal_thresh[i]\n",
    "pfinal_thresh = np.array([0.82, 0.82, 0.85, 0.9])\n",
    "\n",
    "# early stopping criteria\n",
    "pmid_accept = 0.9\n",
    "pmid_fail = 0.05\n",
    "\n",
    "prior_mu_mean = scipy.special.logit(0.1)\n",
    "\n",
    "def berry_prior(theta):\n",
    "    mu = theta[..., 0]\n",
    "    mu_prior = scipy.stats.norm.logpdf(mu, prior_mu_mean, 100)\n",
    "\n",
    "    sigma2 = theta[..., 1]\n",
    "    alpha = 0.0005\n",
    "    beta = 0.000005\n",
    "    sigma2_prior = scipy.stats.invgamma.logpdf(sigma2, alpha, scale=beta)\n",
    "    return mu_prior + sigma2_prior\n",
    "\n",
    "model = inla.binomial_hierarchical()\n",
    "model.log_prior = berry_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "def sim(n_sims, N, p):\n",
    "    return np.stack(\n",
    "        (scipy.stats.binom.rvs(N, p, size=(n_sims, 4)), np.full((n_sims, 4), N)), axis=2\n",
    "    )\n",
    "\n",
    "n_sims = 200\n",
    "\n",
    "scenarios = {\n",
    "    \"Null\": [0.05, 0.05, 0.1, 0.2],\n",
    "    \"Alternative\": [0.2, 0.2, 0.3, 0.4],\n",
    "    \"One in the Middle\": [0.2, 0.2, 0.2, 0.5],\n",
    "    \"All in the Middle\": [0.15, 0.15, 0.2, 0.3],\n",
    "    \"One Nugget\": [0.05, 0.05, 0.1, 0.4],\n",
    "    \"2 Null, 2 Alternative\": [0.05, 0.05, 0.3, 0.4],\n",
    "}\n",
    "\n",
    "# Number of patients at the first look.\n",
    "N_0 = 10\n",
    "\n",
    "# Number of patients added per look.\n",
    "deltaN = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "n_looks = 6\n",
    "for k in scenarios:\n",
    "    results[k] = dict(\n",
    "        data=[], mu_post=[], sigma_post=[], p_success=[], success_by_look=[]\n",
    "    )\n",
    "    p = scenarios[k]\n",
    "    data = sim(n_sims, N_0, p)\n",
    "\n",
    "    results[k][\"data\"] = []\n",
    "    success = np.zeros((n_sims, 4), dtype=bool)\n",
    "    stopping_time = np.full((n_sims, 4), -1, dtype=np.int32)\n",
    "    for look in range(n_looks):\n",
    "        results[k][\"data\"].append(data.copy())\n",
    "\n",
    "        mu_rule = inla.gauss_rule(21, -5, 5)\n",
    "        sigma2_rule = log_gauss_rule(21, 1e-7, 1e3)\n",
    "        post_theta, report = inla.calc_posterior_theta(\n",
    "            model, data, (mu_rule, sigma2_rule)\n",
    "        )\n",
    "        mu_post, sigma_post = inla.calc_posterior_x(post_theta, report)\n",
    "        results[k][\"mu_post\"].append(mu_post)\n",
    "        results[k][\"sigma_post\"].append(sigma_post)\n",
    "\n",
    "        if look < 5:\n",
    "            p_success = 1.0 - scipy.stats.norm.cdf(pmid_x, mu_post, sigma_post)\n",
    "            stop_success = 0 * (p_success > pmid_accept) # no early stopping for success\n",
    "            stop_fail = p_success < pmid_fail\n",
    "        else:\n",
    "            p_success = 1.0 - scipy.stats.norm.cdf(p0_x, mu_post, sigma_post)\n",
    "            stop_success = p_success > pfinal_thresh[None, :]\n",
    "            stop_fail = p_success <= pfinal_thresh[None, :]\n",
    "        success[stop_success] = True\n",
    "        stop = stop_success | stop_fail\n",
    "        update_stopping = (stop & (stopping_time == -1)).astype(bool)\n",
    "        stopping_time[update_stopping] = look\n",
    "\n",
    "        results[k][\"p_success\"].append(p_success)\n",
    "        results[k][\"success_by_look\"].append(stop_success)\n",
    "\n",
    "        new_data = sim(n_sims, deltaN, p)\n",
    "        data += new_data\n",
    "    results[k][\"success_by_look\"] = np.array(results[k][\"success_by_look\"])\n",
    "    results[k]['stopping_time'] = stopping_time\n",
    "    results[k]['success'] = success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), constrained_layout=True)\n",
    "for i, k in enumerate(scenarios):\n",
    "    sample_size = N_0 + results[k]['stopping_time'] * deltaN\n",
    "    mean_sample_size = np.mean(sample_size, axis=0)\n",
    "\n",
    "    plt.subplot(2, 3, 1 + i)\n",
    "    plt.title(k, fontweight='bold')\n",
    "    plt.bar(np.arange(4), mean_sample_size, 0.2, color='gray', )\n",
    "\n",
    "    plt.ylim([0, 35])\n",
    "    plt.yticks(np.arange(0, 36, 5))\n",
    "    plt.ylabel('Pr(Success)')\n",
    "\n",
    "    plt.xticks(np.arange(4), ['1', '2', '3', '4'])\n",
    "    plt.xlabel('Group')\n",
    "\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), constrained_layout=True)\n",
    "for i, k in enumerate(results):\n",
    "    success = results[k]['success']\n",
    "    n_sims = success.shape[0]\n",
    "    success_rate = success.sum(axis=0) / n_sims\n",
    "    plt.subplot(2, 3, 1 + i)\n",
    "    plt.title(k, fontweight='bold')\n",
    "    plt.bar(np.arange(4), success_rate, 0.2, color='gray', )\n",
    "\n",
    "    plt.ylim([0, 1])\n",
    "    plt.yticks(np.linspace(0, 1, 6))\n",
    "    plt.ylabel('Pr(Success)')\n",
    "\n",
    "    plt.xticks(np.arange(4), ['1', '2', '3', '4'])\n",
    "    plt.xlabel('Group')\n",
    "\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9637099bd81b2ef0895c64d539356b45819bc945d59d426757b1f51ae370d50"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('kevlar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
