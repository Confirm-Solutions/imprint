{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating R-INLA results\n",
    "\n",
    "## Joint density and its gradient and Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "import scipy.integrate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import inla\n",
    "\n",
    "nT = 50\n",
    "n_i = np.full(4, nT)\n",
    "x_i = np.array(\n",
    "    [0.0542217351633235, -0.7058565689642957, 0.5411263138456900, 1.1393884075711429]\n",
    ")\n",
    "p_i = np.array(\n",
    "    [0.5135521136895386, 0.3305150325484877, 0.6320743881220601, 0.7575673322021476]\n",
    ")\n",
    "y_i = np.array([28, 14, 33, 36]) * nT / 50\n",
    "rinla_cis = np.array(\n",
    "    [\n",
    "        [-0.2914459, 0.7714731],\n",
    "        [-1.399438, -0.1621846],\n",
    "        [0.05145177, 1.165792],\n",
    "        [0.2554444, 1.437953],\n",
    "    ]\n",
    ")\n",
    "data = np.stack((y_i[None, :], n_i[None, :]), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qv is the value on the precision matrix diagonal. We can either treat Qv as\n",
    "# known and infer the x_i via what would seem to be a purely frequentist maximum\n",
    "# likelihood estimation. Or we can treat Qv as drawn from a prior distribution\n",
    "# and infer the posterior distribution of both x_i and Qv.\n",
    "def calc_precision_matrixQ(Qv):\n",
    "    return np.diag(np.full(x_i.shape[0], Qv))\n",
    "\n",
    "\n",
    "# Assume a normal prior for `a` centered at zero with variance 1 and a lognormal distribution for Qv with shape parameter 10.0.\n",
    "def calc_log_prior(a, Qv):\n",
    "    return scipy.stats.norm.logpdf(a, 0, 1) + scipy.stats.lognorm.logpdf(Qv, 10.0)\n",
    "\n",
    "\n",
    "def calc_log_joint(x, y, n, a, Qv):\n",
    "    Q = calc_precision_matrixQ(Qv)\n",
    "    term1 = -(x - a).T.dot(Q).dot(x - a) / 2\n",
    "    term2 = np.sum(x * y - n * np.log(np.exp(x) + 1))\n",
    "    term3 = np.log(np.linalg.det(Q)) / 2\n",
    "    term4 = calc_log_prior(a, Qv)\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "\n",
    "def calc_log_joint_xonly(x, y, n, a, Qv):\n",
    "    Q = calc_precision_matrixQ(Qv)\n",
    "    term1 = -(x - a).T.dot(Q).dot(x - a) / 2\n",
    "    term2 = np.sum(-n * np.log(np.exp(x) + 1))\n",
    "    term3 = np.sum(x * y)\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "\n",
    "def calc_gradx_log_joint(x, y, n, a, Qv):\n",
    "    Q = calc_precision_matrixQ(Qv)\n",
    "    term1 = -Q.dot(x - a)\n",
    "    term2 = y - (n * np.exp(x) / (np.exp(x) + 1))\n",
    "    return term1 + term2\n",
    "\n",
    "\n",
    "def calc_hessx_log_joint(x, y, n, a, Qv):\n",
    "    Q = calc_precision_matrixQ(Qv)\n",
    "    term1 = -np.diag(n * np.exp(x) / ((np.exp(x) + 1) ** 2))\n",
    "    term2 = -Q\n",
    "    return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_x0_brute(av, Qv0):\n",
    "    return scipy.optimize.minimize(\n",
    "        lambda xvec: -calc_log_joint_xonly(xvec, y_i, n_i, av, Qv0),\n",
    "        np.zeros_like(y_i),\n",
    "        jac=lambda xvec: -calc_gradx_log_joint(xvec, y_i, n_i, av, Qv0),\n",
    "        method=\"BFGS\",\n",
    "    )\n",
    "\n",
    "\n",
    "calc_x0_brute(0.0, 1.0)[\"x\"], x_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inla\n",
    "\n",
    "model = inla.binomial_hierarchical()\n",
    "inla.optimize_x0(model, data, np.array([[0.0, 1.0]]))[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qv0 = 1.0\n",
    "dx = 0.001\n",
    "\n",
    "\n",
    "def calc_numerical_grad(local_x_i, row):\n",
    "    dx_vec = np.zeros(4)\n",
    "    dx_vec[row] = dx\n",
    "    f0 = calc_log_joint(local_x_i - dx_vec, y_i, n_i, 0, qv0)\n",
    "    f2 = calc_log_joint(local_x_i + dx_vec, y_i, n_i, 0, qv0)\n",
    "    f0_xonly = calc_log_joint_xonly(local_x_i - dx_vec, y_i, n_i, 0, qv0)\n",
    "    f2_xonly = calc_log_joint_xonly(local_x_i + dx_vec, y_i, n_i, 0, qv0)\n",
    "\n",
    "    # check that xonly is only dropping terms independent of x\n",
    "    np.testing.assert_allclose(f2 - f0, f2_xonly - f0_xonly)\n",
    "    return (f2 - f0) / (2 * dx)\n",
    "\n",
    "\n",
    "num_grad = np.empty(4)\n",
    "for i in range(4):\n",
    "    num_grad[i] = calc_numerical_grad(x_i, i)\n",
    "analytical_grad = calc_gradx_log_joint(x_i, y_i, n_i, 0, qv0)\n",
    "\n",
    "num_hess = np.empty((4, 4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        dx_vec = np.zeros(4)\n",
    "        dx_vec[j] = dx\n",
    "        g0 = calc_numerical_grad(x_i - dx_vec, i)\n",
    "        g2 = calc_numerical_grad(x_i + dx_vec, i)\n",
    "        num_hess[i, j] = (g2 - g0) / (2 * dx)\n",
    "np.set_printoptions(linewidth=100)\n",
    "analytical_hess = calc_hessx_log_joint(x_i, y_i, n_i, 0, qv0)\n",
    "\n",
    "np.testing.assert_allclose(num_grad, analytical_grad, atol=1e-5)\n",
    "np.testing.assert_allclose(num_hess, analytical_hess, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate\n",
    "\n",
    "x_mle = calc_x0_brute(0, 1.0)\n",
    "for j in range(4):\n",
    "    xj_vals = np.linspace(-2, 2, 200)\n",
    "    x_vals = np.empty((200, 4))\n",
    "    x_vals[:, :] = x_mle[\"x\"]\n",
    "    x_vals[:, j] = xj_vals\n",
    "    density_xj = np.array(\n",
    "        [\n",
    "            np.exp(calc_log_joint_xonly(x_vals[i], y_i, n_i, 0, 1.0))\n",
    "            for i in range(x_vals.shape[0])\n",
    "        ]\n",
    "    )\n",
    "    scaled = density_xj  # / laplace_I\n",
    "    # plt.plot(xj_vals, scaled)\n",
    "    # plt.show()\n",
    "\n",
    "    cdf = np.array(\n",
    "        [\n",
    "            scipy.integrate.simpson(scaled[:i], xj_vals[:i])\n",
    "            for i in range(1, xj_vals.shape[0])\n",
    "        ]\n",
    "    )\n",
    "    normalize = cdf[-1]\n",
    "    cdf /= normalize\n",
    "\n",
    "    # plt.plot(xj_vals[1:], cdf)\n",
    "    # plt.show()\n",
    "    ci = (xj_vals[np.argmax(cdf > 0.025) - 1], xj_vals[np.argmax(cdf > 0.975) + 1])\n",
    "    print(f\"x[{j}] ci here: {ci} \\nr-inla ci: {rinla_cis[j]}  \\ntrue val: {x_i[j]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the maximum of the joint density as a function of $x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: derive expression for hyperparameters:\n",
    "\\begin{align}\n",
    "\\log p(y, x, \\theta) = \\log p(x|\\theta, y) + \\log p(\\theta|y) + \\log p(y)\\\\\n",
    "\\log p(\\theta | y) ~~ \\propto ~~ \\log p(y, x_0, \\theta) - \\log p(x_0|y, \\theta)\\\\\n",
    "\\end{align}\n",
    "\n",
    "step 2: laplace approximation for handling the latent variables\n",
    "\\begin{align}\n",
    "f(x) \\propto  \\log p(y, x, \\theta)\\\\\n",
    "\\log p(x_0 | y, \\theta) = \\log \\big[ \\frac{\\exp [f(x_0)]}{LA(\\int exp[f(x)] dx)} \\big]\\\\\n",
    "\\log p(x_0 | y, \\theta) = \\log e^{f(x_0)} - \\log LA(\\int exp[f(x)] dx) \\big]\\\\\n",
    "\\log p(x_0 | y, \\theta) = \\log e^{f(x_0)} - \\log \\big[ e^{f(x_0)} (2\\pi)^{D/2}|-Hf(x_0))|^{-1/2} \\big]\\\\\n",
    "\\log p(x_0 | y, \\theta) = \\log e^{f(x_0)} - \\log e^{f(x_0)} - \\log \\big[(2\\pi)^{D/2}\\big] - \\log \\Big[|-Hf(x_0)|^{-1/2}\\Big]\\\\\n",
    "\\log p(x_0 | y, \\theta) = -(D/2)\\log (2\\pi) + \\frac{1}{2}\\log \\Big[|-Hf(x_0)|\\Big]\\\\\n",
    "\\end{align}\n",
    "\n",
    "step 3: combine\n",
    "\\begin{align}\n",
    "\\log p(\\theta | y) ~~ \\propto ~~ \\log p(y, x_0, \\theta) + (D/2)\\log (2\\pi) - \\frac{1}{2}\\log \\Big[|-Hf(x_0)|\\Big]\\\\\n",
    "\\log p(\\theta | y) ~~ \\propto ~~ \\log p(y, x_0, \\theta) - \\frac{1}{2} \\log |-Hf(x_0)|\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_posterior_theta(a, Qv):\n",
    "    x0_info = calc_x0_brute(a, Qv)\n",
    "    detnegH = np.linalg.det(-calc_hessx_log_joint(x0_info[\"x\"], y_i, n_i, a, Qv))\n",
    "    ljoint = calc_log_joint(x0_info[\"x\"], y_i, n_i, a, Qv)\n",
    "    return ljoint - 0.5 * np.log(detnegH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vals = np.linspace(-2, 2, 30)\n",
    "q_vals = np.linspace(0.25, 10.0, 20)\n",
    "AA, QQ = np.meshgrid(a_vals, q_vals)\n",
    "\n",
    "logpost = np.array(\n",
    "    [[calc_log_posterior_theta(av, qv) for av in a_vals] for qv in q_vals]\n",
    ")\n",
    "logpost -= np.mean(logpost)\n",
    "post_theta = np.exp(logpost)\n",
    "post_theta_normalization = scipy.integrate.simpson(\n",
    "    scipy.integrate.simpson(post_theta, a_vals), q_vals\n",
    ")\n",
    "post_theta /= post_theta_normalization\n",
    "# print(a_vals[np.argmax(logpost)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = np.linspace(0, 0.5, 21)\n",
    "cntf = plt.contourf(AA, 1 / QQ, post_theta.reshape(AA.shape), levels=levels)\n",
    "plt.contour(\n",
    "    AA,\n",
    "    1 / QQ,\n",
    "    post_theta.reshape(AA.shape),\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    linewidths=0.5,\n",
    "    levels=levels,\n",
    ")\n",
    "cbar = plt.colorbar(cntf)\n",
    "plt.xlabel(\"$a$\")\n",
    "plt.ylabel(\"$1/Q_v$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.argmax(logpost), logpost.shape)\n",
    "map_hyperparams = AA[idx], 2 ** QQ[idx]\n",
    "map_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior of the latent variables\n",
    "$$p(x|y) = \\int p(x|y,\\theta) p(\\theta | y) d\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1000**0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_px_given_ytheta(xj, j, a, Qv):\n",
    "    # laplace approximation of denomiator/scaling\n",
    "    x0_info = calc_x0_brute(a, Qv)\n",
    "    detnegH = np.linalg.det(-calc_hessx_log_joint(x0_info[\"x\"], y_i, n_i, a, Qv))\n",
    "    laplace_I = (\n",
    "        np.exp(-x0_info[\"fun\"]) * ((2 * np.pi) ** (y_i.shape[0] / 2)) / np.sqrt(detnegH)\n",
    "    )\n",
    "\n",
    "    x = x0_info[\"x\"].copy()\n",
    "    x[j] = xj\n",
    "    density_x = calc_log_joint(x, y_i, n_i, a, Qv)\n",
    "    scaled = np.exp(density_x) / laplace_I\n",
    "    return scaled\n",
    "\n",
    "\n",
    "xj_vals = np.linspace(-2, 2, 60)\n",
    "for j in range(4):\n",
    "    px_given_ytheta = np.array(\n",
    "        [\n",
    "            [[calc_px_given_ytheta(xv, j, av, qv) for xv in xj_vals] for av in a_vals]\n",
    "            for qv in q_vals\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    post_x_terms = px_given_ytheta * post_theta[:, :, None]\n",
    "    post_x = scipy.integrate.simpson(\n",
    "        scipy.integrate.simpson(post_x_terms, q_vals, axis=0), a_vals, axis=0\n",
    "    )\n",
    "    cdf = np.array(\n",
    "        [\n",
    "            scipy.integrate.simpson(post_x[:i], xj_vals[:i])\n",
    "            for i in range(1, xj_vals.shape[0])\n",
    "        ]\n",
    "    )\n",
    "    normalize = cdf[-1]\n",
    "    cdf /= normalize\n",
    "    ci = (xj_vals[np.argmax(cdf > 0.025) - 1], xj_vals[np.argmax(cdf > 0.975) + 1])\n",
    "    print(f\"x[{j}] ci here: {ci} \\nr-inla ci: {rinla_cis[j]}  \\ntrue val: {x_i[j]}\\n\")\n",
    "    print(xj_vals[np.argmax(post_x)])\n",
    "    plt.plot(xj_vals, post_x)\n",
    "    # plt.ylabel('$$')\n",
    "    plt.xlabel(f\"$x_{j}$\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9637099bd81b2ef0895c64d539356b45819bc945d59d426757b1f51ae370d50"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('kevlar': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
