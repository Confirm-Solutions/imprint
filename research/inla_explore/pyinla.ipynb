{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log joint distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be super anal and carefully using symbolic algebra to derive correct terms. I always get constants and signs wrong and this will probably save me some time in the medium run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.var(\"x\", real=True)\n",
    "y = sp.var(\"y\", real=True, positive=True)\n",
    "n = sp.var(\"n\", real=True, positive=True)\n",
    "p = sp.var(\"p\", real=True, positive=True)\n",
    "logit = sp.log(p / (1 - p))\n",
    "display(logit)\n",
    "inv_logit = 1 / (1 + sp.exp(-x))\n",
    "\n",
    "# check inverse logit\n",
    "assert (inv_logit.subs(x, logit)).simplify() == p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_L_p = p**y * (1 - p) ** (n - y)\n",
    "binomial_LL_p = y * sp.log(p) + (n - y) * sp.log(1 - p)\n",
    "\n",
    "# check binomial LL\n",
    "assert (sp.log(binomial_L_p) - binomial_LL_p).simplify().subs(\n",
    "    [(p, 0.5), (n, 10), (y, 6)]\n",
    ") < 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_LL = binomial_LL_p.subs(p, inv_logit).simplify()\n",
    "\n",
    "binomial_LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Copying over stuff from the [INLA Approaches doc](https://docs.google.com/document/d/1RDrGK3Pc-n5BOD2YA0DB702VY2uNEhJ_ur5UrlgVzrI/edit) with Mike and from the [INLA from scratch](https://stefansiegert.net/inla-project/inla-from-scratch) page.\n",
    "\n",
    "The log joint distribution!\n",
    "\\begin{equation}\n",
    "\\log p(y, x, \\theta) = \\log p(y | x,\\theta) + \\log p(x|\\theta) + \\log p(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Three terms:\n",
    "1. The data distribution (likelihood).\n",
    "2. The latent variable distribution. \n",
    "3. The priors. \n",
    "\n",
    "## Likelihood term\n",
    "\n",
    "The first likelihood term is simple and is the typical binomial log likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "L_j(p) = \\binom{n}{y_j} p^{y_j} (1 - p)^{n-y_j} \\\\ \n",
    "\\log L_j(p) = \\log \\binom{n}{y_j} + y_j \\log p + (n - y_j) \\log (1 - p) \n",
    "\\end{align}\n",
    "\n",
    "dropping the first additive constant term and summing over $J$ groups: \n",
    "\n",
    "\\begin{equation}\n",
    "\\log L(p) = \\sum_j^J y_j \\log p_j + (n - y_j) \\log (1 - p_j) \n",
    "\\end{equation}\n",
    "\n",
    "Replacing $p_j$ with inverse logit:\n",
    "\\begin{equation}\n",
    "\\log L(p) = \\sum_j^J y_j x_j - n_j \\log (1 + e^{x_j}) \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Latent variable term\n",
    "\n",
    "\n",
    "The latent variables are assumed to be drawn like:\n",
    "\n",
    "\\begin{equation}\n",
    "x \\sim N(\\mu = a, \\Sigma = Q^{-1})\n",
    "\\end{equation}\n",
    "\n",
    "with the hyperparameters then being $\\theta=\\{a, Q\\}$.\n",
    "\n",
    "So, assuming that Q is chosen and will not be tuned:\n",
    "\\begin{equation}\n",
    "\\log p(x|\\theta) = \\frac{1}{2}\\log |Q|-\\frac{1}{2}(x - a)^TQ(x-a)\n",
    "\\end{equation}\n",
    "We will drop the additive\n",
    "\n",
    "## The whole thing\n",
    "Up to additive constants of parameters that we will not optimize: \n",
    "\\begin{equation}\n",
    "\\log p(y, x, \\theta) = \\sum_j^J y_j x_j - n_j\\log (1 + e^{x_j}) + \\frac{1}{2}\\log |Q| -\\frac{1}{2} (x - a)^T Q (x-a) + \\log p(\\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sp.var(\"a\")\n",
    "Q = sp.var(\"Q\")\n",
    "Qd = sp.var(\"Q_{det}\")\n",
    "# for the sympy derivation, just assume 1D\n",
    "normal_LL = sp.Rational(1, 2) * sp.log(Qd) - sp.Rational(1, 2) * ((x - a) * Q * (x - a))\n",
    "normal_LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_LD = normal_LL + binomial_LL\n",
    "full_LD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace approximation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute:\n",
    "\\begin{equation}\n",
    "\\int_{-10}^{10} e^{\\frac{sin(x)}{x}} dx\n",
    "\\end{equation}\n",
    "with both Gaussian quadrature and with Laplace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "width = 10\n",
    "f = 3 * sp.sin(x) / x\n",
    "test_f = lambda xs: np.array([float(f.subs(x, xv)) for xv in xs])\n",
    "points, weights = np.polynomial.legendre.leggauss(40)\n",
    "mapped_x = points * width\n",
    "mapped_w = weights * width\n",
    "print(\"true val\", np.sum(mapped_w * np.exp(test_f(mapped_x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = float(sp.limit(f, x, 0))\n",
    "# use sp.limit instead of subs since the derivative at x=0 involves some\n",
    "# division by zero in the form derived via sympy\n",
    "fdd0 = float(sp.limit((sp.diff(sp.diff(f))), x, 0))\n",
    "integral = np.exp(f0) * np.sqrt(2 * np.pi / np.abs(fdd0))\n",
    "print(\"laplace appx val: \", integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xplt = np.linspace(-10, 10, 100)\n",
    "true_f = np.exp(test_f(xplt))\n",
    "quadratic = np.exp(f0 - 0.5 * np.abs(fdd0) * xplt**2)\n",
    "plt.plot(xplt, true_f, \"b-\")\n",
    "plt.plot(xplt, quadratic, \"r-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do a quadratic/gaussian approximation for Laplace approximation, we need derivatives of the joint density as a function of the latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_LD = sp.diff(full_LD, x)\n",
    "hess_LD_ugly = sp.diff(grad_LD, x)\n",
    "display(grad_LD)\n",
    "display(hess_LD_ugly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess_LD = -Q - (n * sp.exp(x) / ((sp.exp(x) + 1) ** 2))\n",
    "(hess_LD - hess_LD_ugly).simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above derivation is only correct for a problem with a single x. Extending by hand to the matrix setting:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\log p(x, y, \\theta) \\\\\n",
    "\\nabla f(x) = vec \\big[ y_i - \\frac{n_ie^{x_i}}{e^{x_i} + 1}\\big] - Q(x-a)\\\\\n",
    "Hf(x) = -Q - diag \\big[ \\frac{n_i * e^{x_i}}{(e^{x_i} + 1)^2}]\n",
    "\\end{align}\n",
    "\n",
    "Or more usefully from a code perspective, if the terms in $\\log p(y | x, \\theta)$ are separable and we drop terms that are constant in $x$:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= \\log p(y | x, \\theta) + \\log p(x | \\theta)\\\\\n",
    "f(x) &= g(x, \\theta) - \\frac{1}{2} (x - a)^TQ(x - a)\\\\\n",
    "\\nabla f(x) &= vec \\big[ \\frac{dg(x_i)}{dx_i}\\big] - Q(x-a)\\\\\n",
    "Hf(x) &= diag \\big[ \\frac{d^2g(x_i)}{dx_i^2}\\big] -Q\n",
    "\\end{align}\n",
    "This is nice because it leaves the specification of the model transforming from $x_i$ to $y_i$ unspecified and flexible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating R-INLA results\n",
    "\n",
    "## Joint density and its gradient and Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each $x_i$ is an i.i.d. draw, we know that $Q$ is diagonal. That is, there is no interdependence between $x_i$ and $x_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "n_i = np.full(4, 50)\n",
    "x_i = np.array(\n",
    "    [0.0542217351633235, -0.7058565689642957, 0.5411263138456900, 1.1393884075711429]\n",
    ")\n",
    "p_i = np.array(\n",
    "    [0.5135521136895386, 0.3305150325484877, 0.6320743881220601, 0.7575673322021476]\n",
    ")\n",
    "y_i = np.array([28, 14, 33, 36])\n",
    "rinla_cis = np.array(\n",
    "    [\n",
    "        [-0.2914459, 0.7714731],\n",
    "        [-1.399438, -0.1621846],\n",
    "        [0.05145177, 1.165792],\n",
    "        [0.2554444, 1.437953],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that if you want to ignore the Q_det term, it's simple to do so by just\n",
    "# setting it equal to one. Then, the log Q_det = 0. This can be nice for\n",
    "# optimizing over x.\n",
    "calc_log_joint_sp = sp.lambdify([x, y, n, a, Q, Qd], full_LD, \"numpy\")\n",
    "calc_log_joint_sp(x_i, y_i, n_i, 0, 1.0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qv is the value on the precision matrix diagonal. We can either treat Qv as\n",
    "# known and infer the x_i via what would seem to be a purely frequentist maximum\n",
    "# likelihood estimation. Or we can treat Qv as drawn from a prior distribution\n",
    "# and infer the posterior distribution of both x_i and Qv.\n",
    "def calc_precision_matrixQ(Qv):\n",
    "    return np.diag(np.full(x_i.shape[0], Qv))\n",
    "\n",
    "\n",
    "def calc_log_joint(x, y, n, a, Q):\n",
    "    term1 = -(x - a).T.dot(Q).dot(x - a) / 2\n",
    "    term2 = np.sum(-n * np.log(np.exp(x) + 1))\n",
    "    term3 = np.sum(x * y)\n",
    "    term4 = np.log(np.linalg.det(Q)) / 2\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "\n",
    "def calc_log_joint_xonly(x, y, n, a, Q):\n",
    "    term1 = -(x - a).T.dot(Q).dot(x - a) / 2\n",
    "    term2 = np.sum(-n * np.log(np.exp(x) + 1))\n",
    "    term3 = np.sum(x * y)\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "\n",
    "def calc_gradx_log_joint(x, y, n, a, Q):\n",
    "    term1 = -Q.dot(x - a)\n",
    "    term2 = -(n * np.exp(x) / (np.exp(x) + 1))\n",
    "    term3 = y\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "\n",
    "def calc_hessx_log_joint(x, y, n, a, Q):\n",
    "    term1 = -np.diag(n * np.exp(x) / ((np.exp(x) + 1) ** 2))\n",
    "    term2 = -Q\n",
    "    return term1 + term2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that gradient and hessian numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qv0 = 1.0\n",
    "Qmat = calc_precision_matrixQ(Qv0)\n",
    "dx = 0.001\n",
    "\n",
    "\n",
    "def calc_numerical_grad(local_x_i, row):\n",
    "    dx_vec = np.zeros(4)\n",
    "    dx_vec[row] = dx\n",
    "    f0 = calc_log_joint_xonly(local_x_i - dx_vec, y_i, n_i, 0, Qmat)\n",
    "    f2 = calc_log_joint_xonly(local_x_i + dx_vec, y_i, n_i, 0, Qmat)\n",
    "    return (f2 - f0) / (2 * dx)\n",
    "\n",
    "\n",
    "num_grad = np.empty(4)\n",
    "for i in range(4):\n",
    "    num_grad[i] = calc_numerical_grad(x_i, i)\n",
    "analytical_grad = calc_gradx_log_joint(x_i, y_i, n_i, 0, Qmat)\n",
    "num_grad, analytical_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hess = np.empty((4, 4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        dx_vec = np.zeros(4)\n",
    "        dx_vec[j] = dx\n",
    "        g0 = calc_numerical_grad(x_i + dx_vec, i)\n",
    "        g2 = calc_numerical_grad(x_i - dx_vec, i)\n",
    "        num_hess[i, j] = (g2 - g0) / (2 * dx)\n",
    "np.set_printoptions(linewidth=100)\n",
    "analytical_hess = calc_hessx_log_joint(x_i, y_i, n_i, a, Qmat)\n",
    "num_hess, analytical_hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the maximum of the joint density as a function of $x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "# Remember that x_i is unknown here since we only observe y_i and n_i. a and Qmat are assumed.\n",
    "def calc_x0_brute(av, Qmat):\n",
    "    return scipy.optimize.minimize(\n",
    "        lambda xvec: -calc_log_joint_xonly(xvec, y_i, n_i, av, Qmat),\n",
    "        np.zeros_like(y_i),\n",
    "        jac=lambda xvec: -calc_gradx_log_joint(xvec, y_i, n_i, av, Qmat),\n",
    "        method=\"BFGS\"\n",
    "        # hess=lambda xvec: calc_hessx_log_joint(xvec, y_i, n_i, 0, Qmat),\n",
    "        # method='Newton-CG'\n",
    "    )\n",
    "\n",
    "\n",
    "x_max = calc_x0_brute(0, Qmat)\n",
    "x_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I *think* that this is the maximum likelihood estimate for $x$. In which case it should track somewhat with the true `x_i` array. And it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i, y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradx_max = calc_gradx_log_joint(x_max[\"x\"], y_i, n_i, 0, Qmat)\n",
    "np.testing.assert_allclose(gradx_max, 0, atol=1e-5)\n",
    "gradx_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessx_max = calc_hessx_log_joint(x_max[\"x\"], y_i, n_i, 0, Qmat)\n",
    "hessx_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic approximation of the log density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appx_density(pts, x0, fx0):\n",
    "    hessx_max = calc_hessx_log_joint(x0, y_i, n_i, 0, Qmat)\n",
    "    x_deviation = pts - x0\n",
    "    return -fx0 + 0.5 * np.sum(x_deviation.dot(hessx_max) * x_deviation, axis=1)\n",
    "\n",
    "\n",
    "pltx = np.linspace(x_max[\"x\"][0] - 1, x_max[\"x\"][0] + 1, 50)\n",
    "plty = np.linspace(x_max[\"x\"][1] - 1, x_max[\"x\"][1] + 1, 50)\n",
    "pltX, pltY = np.meshgrid(pltx, plty)\n",
    "plt_pts = np.stack((pltX, pltY), axis=2).reshape((-1, 2))\n",
    "plt_pts_full = np.empty((plt_pts.shape[0], 4))\n",
    "plt_pts_full[:, :2] = plt_pts\n",
    "plt_pts_full[:, 2:] = x_max[\"x\"][2:]\n",
    "density = np.array(\n",
    "    [\n",
    "        calc_log_joint_xonly(\n",
    "            plt_pts_full[i], y_i[None, None, :], n_i[None, None, :], 0, Qmat\n",
    "        )\n",
    "        for i in range(plt_pts_full.shape[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "x_deviation = plt_pts_full - x_max[\"x\"]\n",
    "density_quadratic = appx_density(plt_pts_full, x_max[\"x\"], x_max[\"fun\"])\n",
    "error = density - density_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5), constrained_layout=True)\n",
    "plt.subplot(1, 3, 1)\n",
    "levels = np.linspace(np.min(density), np.max(density), 21)\n",
    "cntf = plt.contourf(pltX, pltY, density.reshape(pltX.shape), levels=levels)\n",
    "plt.contour(\n",
    "    pltX,\n",
    "    pltY,\n",
    "    density.reshape(pltX.shape),\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    linewidths=0.5,\n",
    "    levels=levels,\n",
    ")\n",
    "plt.plot([x_max[\"x\"][0]], [x_max[\"x\"][1]], \"ro\")\n",
    "cbar = plt.colorbar(cntf)\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "cbar.set_label(\"density\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "cntf = plt.contourf(pltX, pltY, density_quadratic.reshape(pltX.shape), levels=levels)\n",
    "plt.contour(\n",
    "    pltX,\n",
    "    pltY,\n",
    "    density_quadratic.reshape(pltX.shape),\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    linewidths=0.5,\n",
    "    levels=levels,\n",
    ")\n",
    "plt.plot([x_max[\"x\"][0]], [x_max[\"x\"][1]], \"ro\")\n",
    "cbar = plt.colorbar(cntf)\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "cbar.set_label(\"quadratic\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "cntf = plt.contourf(pltX, pltY, error.reshape(pltX.shape))\n",
    "plt.contour(\n",
    "    pltX, pltY, error.reshape(pltX.shape), colors=\"k\", linestyles=\"-\", linewidths=0.5\n",
    ")\n",
    "plt.plot([x_max[\"x\"][0]], [x_max[\"x\"][1]], \"ro\")\n",
    "cbar = plt.colorbar(cntf)\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "cbar.set_label(\"error\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from log-p-space to p-space. I also offset since these are\n",
    "# un-normalized and, in log space, any constant can be legally added or\n",
    "# subtracted.\n",
    "density_offset = np.max(density)\n",
    "exp_density = np.exp(density - density_offset)\n",
    "exp_density_quadratic = np.exp(density_quadratic - density_offset)\n",
    "exp_error = exp_density - exp_density_quadratic\n",
    "\n",
    "plt.figure(figsize=(15, 5), constrained_layout=True)\n",
    "plt.subplot(1, 3, 1)\n",
    "levels = np.linspace(np.min(exp_density), np.max(exp_density), 21)\n",
    "cntf = plt.contourf(pltX, pltY, exp_density.reshape(pltX.shape), levels=levels)\n",
    "plt.contour(\n",
    "    pltX,\n",
    "    pltY,\n",
    "    exp_density.reshape(pltX.shape),\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    linewidths=0.5,\n",
    "    levels=levels,\n",
    ")\n",
    "plt.plot([x_max[\"x\"][0]], [x_max[\"x\"][1]], \"ro\")\n",
    "cbar = plt.colorbar(cntf)\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "cbar.set_label(\"density\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "cntf = plt.contourf(\n",
    "    pltX, pltY, exp_density_quadratic.reshape(pltX.shape), levels=levels\n",
    ")\n",
    "plt.contour(\n",
    "    pltX,\n",
    "    pltY,\n",
    "    exp_density_quadratic.reshape(pltX.shape),\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    linewidths=0.5,\n",
    "    levels=levels,\n",
    ")\n",
    "plt.plot([x_max[\"x\"][0]], [x_max[\"x\"][1]], \"ro\")\n",
    "cbar = plt.colorbar(cntf)\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "cbar.set_label(\"quadratic\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "cntf = plt.contourf(pltX, pltY, exp_error.reshape(pltX.shape))\n",
    "plt.contour(\n",
    "    pltX,\n",
    "    pltY,\n",
    "    exp_error.reshape(pltX.shape),\n",
    "    colors=\"k\",\n",
    "    linestyles=\"-\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "plt.plot([x_max[\"x\"][0]], [x_max[\"x\"][1]], \"ro\")\n",
    "cbar = plt.colorbar(cntf)\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "cbar.set_label(\"error\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's integrate the true joint density with a Gaussian quadrature rule. This is impractical for a larger problem, but it works just fine for a 4D problem. Then, we'll compare with the Laplace approximation integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgaussx, qgaussw = np.polynomial.legendre.leggauss(10)\n",
    "\n",
    "qx0, qx1, qx2, qx3 = np.meshgrid(qgaussx, qgaussx, qgaussx, qgaussx)\n",
    "qx = np.stack((qx0, qx1, qx2, qx3), axis=4).reshape((-1, 4))\n",
    "qx += x_max[\"x\"]\n",
    "qw = (\n",
    "    qgaussw[None, None, None, :]\n",
    "    * qgaussw[None, None, :, None]\n",
    "    * qgaussw[None, :, None, None]\n",
    "    * qgaussw[:, None, None, None]\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(qx, axis=0), np.max(qx, axis=0), np.sum(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_qx = np.array(\n",
    "    [np.exp(calc_log_joint_xonly(qx[i], y_i, n_i, 0, Qmat)) for i in range(qx.shape[0])]\n",
    ")\n",
    "gauss_I = np.sum(qw * density_qx)\n",
    "laplace_I = (\n",
    "    np.exp(-x_max[\"fun\"])\n",
    "    * ((2 * np.pi) ** (y_i.shape[0] / 2))\n",
    "    / np.sqrt(np.linalg.det(-hessx_max))\n",
    ")\n",
    "gauss_I, laplace_I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dang**!!! Pretty good!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a slice of $p(x|y,\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll plot for various values of $x_0$ while holding $x_{1..3}$ fixed and equal to the MLE estimate.\n",
    "\n",
    "I'm a bit suspicious about some of this since I'm using a joint density function between all $x_{0..3}$ and then just normalizing the \"cdf\". I think this is wrong since I should be integrating out the other variables to get a marginal distribution for a single $x_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate\n",
    "\n",
    "for j in range(4):\n",
    "    xj_vals = np.linspace(-2, 2, 200)\n",
    "    x_vals = np.empty((200, 4))\n",
    "    x_vals[:, :] = x_max[\"x\"]\n",
    "    x_vals[:, j] = xj_vals\n",
    "    density_xj = np.array(\n",
    "        [\n",
    "            np.exp(calc_log_joint_xonly(x_vals[i], y_i, n_i, 0, Qmat))\n",
    "            for i in range(x_vals.shape[0])\n",
    "        ]\n",
    "    )\n",
    "    scaled = density_xj / laplace_I\n",
    "    # plt.plot(xj_vals, scaled)\n",
    "    # plt.show()\n",
    "\n",
    "    cdf = np.array(\n",
    "        [\n",
    "            scipy.integrate.simpson(scaled[:i], xj_vals[:i])\n",
    "            for i in range(1, xj_vals.shape[0])\n",
    "        ]\n",
    "    )\n",
    "    normalize = cdf[-1]\n",
    "    cdf /= normalize\n",
    "\n",
    "    # plt.plot(xj_vals[1:], cdf)\n",
    "    # plt.show()\n",
    "    ci = (xj_vals[np.argmax(cdf > 0.025) - 1], xj_vals[np.argmax(cdf > 0.975) + 1])\n",
    "    print(f\"x[{j}] ci here: {ci} \\nr-inla ci: {rinla_cis[j]}  \\ntrue val: {x_i[j]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better marginal distribution."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9637099bd81b2ef0895c64d539356b45819bc945d59d426757b1f51ae370d50"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('kevlar': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
