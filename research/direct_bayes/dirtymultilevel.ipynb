{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1\n",
    "\n",
    "Fixed hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helper functions\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "\n",
    "\n",
    "def invlogit(theta):\n",
    "    return 1 / (1 + np.exp(-theta))\n",
    "\n",
    "\n",
    "\n",
    "d = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dirtymultilevel(\n",
    "    n=np.repeat(100, d), phat=np.repeat(0.5, d), mu0=np.repeat(0, d), V0=None, return_ci=False\n",
    "):\n",
    "    if V0 is None:\n",
    "        # note: V0 should generally be equal to the prior sigma-inverse\n",
    "        V0 = np.diag(np.repeat(1, len(phat)))\n",
    "    # phat += 1e-8\n",
    "    thetahat = logit(phat)\n",
    "    sample_I = np.diag(n * phat * (1 - phat))\n",
    "    precision_posterior = V0 + sample_I\n",
    "    Sigma_posterior = np.linalg.inv(precision_posterior)\n",
    "    mu_posterior = Sigma_posterior @ (sample_I @ thetahat + V0 @ mu0)\n",
    "    # mu_posterior = np.linalg.solve(precision_posterior, sample_I @ thetahat + V0 @ mu0)\n",
    "    ci_dict = {}\n",
    "    if return_ci:\n",
    "        # implement 95% CI on each arm\n",
    "        CI_upper_logit = mu_posterior + 1.96 * np.sqrt(np.diag(Sigma_posterior))\n",
    "        median_logit = mu_posterior\n",
    "        CI_lower_logit = mu_posterior - 1.96 * np.sqrt(np.diag(Sigma_posterior))\n",
    "        conf_logit = pd.DataFrame(\n",
    "            np.column_stack([CI_lower_logit, median_logit, CI_upper_logit]),\n",
    "            columns=(\"lower\", \"median\", \"upper\"),\n",
    "        )\n",
    "        conf_prob = conf_logit.apply(invlogit, axis=1)\n",
    "        ci_dict['conf_logit']=conf_logit\n",
    "        ci_dict['conf_prob']=conf_prob\n",
    "    return dict(\n",
    "        mu_posterior=mu_posterior,\n",
    "        Sigma_posterior=Sigma_posterior,\n",
    "        **ci_dict,\n",
    "    )\n",
    "\n",
    "# dirtymultilevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2\n",
    "\n",
    "First attempt at integrating the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "p_threshold = np.repeat(0.4, d)\n",
    "\n",
    "\n",
    "def fast_invert(S, d):\n",
    "    for k in range(len(d)):\n",
    "        offset = (d[k] / (1 + d[k] * S[k, k])) * np.outer(\n",
    "            S[k],\n",
    "            S[..., k],\n",
    "        )  # I wonder how to cheaply represent this outer? but in C++ it should be just a trivial for-loop anyway\n",
    "        S = S - offset\n",
    "    return S\n",
    "\n",
    "\n",
    "# TODO: vectorize this in terms of sigma_sq\n",
    "def conditional_exceed_prob_given_sigma(\n",
    "    sigma_sq: float,\n",
    "    mu_sig_sq: float,\n",
    "    thresh=logit(p_threshold),\n",
    "    n=np.repeat(100, d),\n",
    "    phat=np.repeat(0.5, d),\n",
    "    mu_0=np.repeat(0, d),\n",
    "):\n",
    "    assert len(sigma_sq) == 1, sigma_sq\n",
    "    S_0 = np.diag(np.repeat(sigma_sq, d)) + mu_sig_sq\n",
    "    # V_0 = solve(S_0) #but because this is a known case of the form aI + bJ, we can use the explicit\n",
    "    # inverse formula, given by: 1/a I - J*(b/(a(a+db)))\n",
    "    V_0 = np.diag(np.repeat(1 / sigma_sq, d)) - (mu_sig_sq / sigma_sq) / (\n",
    "        sigma_sq + d * mu_sig_sq\n",
    "    )  # Note, by the way, that it's probably possible to use significant precomputation here\n",
    "    thetahat = logit(phat)\n",
    "    sample_I = n * phat * (1 - phat)  # diag(n*phat*(1-phat))\n",
    "    Sigma_posterior = fast_invert(S_0, sample_I)\n",
    "    # precision_posterior = V_0 + np.diag(sample_I)\n",
    "    # Sigma_posterior = np.linalg.inv(precision_posterior)\n",
    "    mu_posterior = Sigma_posterior @ (sample_I * thetahat + V_0 @ mu_0)\n",
    "\n",
    "    # What we now must return instead, is posterior threshold exceedance probabilities for each arm.\n",
    "    z_scores = (mu_posterior - thresh) / np.sqrt(np.diag(Sigma_posterior))\n",
    "    # return(list( thresh_exceed = thresh_exceed, mu_posterior = mu_posterior, Sigma_posterior = Sigma_posterior ))\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "n = 50\n",
    "# let's evaluate the endpoints of the prior in logspace-sigma:\n",
    "# determine endpoints:\n",
    "def get_false_rejections():\n",
    "    a = np.log(1e-8)\n",
    "    b = np.log(1e3)\n",
    "\n",
    "    pts, wts = np.polynomial.legendre.leggauss(n)\n",
    "    pts = (pts + 1) * (b - a) / 2 + a\n",
    "    wts = wts * (b - a) / 2  # sum(wts) = b-a so it averages to 1 over space\n",
    "    density_logspace = scipy.stats.invgamma.pdf(\n",
    "        np.exp(pts), a=0.0005, scale=0.000005\n",
    "    ) * np.exp(pts)\n",
    "    exceed_probs = np.apply_along_axis(\n",
    "        conditional_exceed_prob_given_sigma,\n",
    "        0,\n",
    "        np.exp(pts)[np.newaxis],\n",
    "        mu_sig_sq=0.1,\n",
    "        n=np.repeat(50, d),\n",
    "        phat=np.array([28, 14, 33, 36]) / 50,\n",
    "        mu_0=np.repeat(0, d),\n",
    "    )\n",
    "\n",
    "    contributions = (\n",
    "        exceed_probs.T\n",
    "        * density_logspace[:, np.newaxis]\n",
    "        * wts[:, np.newaxis]\n",
    "        / np.sum(density_logspace * wts)\n",
    "    )  # this automatically fills in density_logspace by column\n",
    "    # Now, the final step is to sum\n",
    "    posterior_exceedance_prob = np.sum(contributions, 0)\n",
    "    return posterior_exceedance_prob\n",
    "\n",
    "\n",
    "expected = [3.18017917, 1.04572162, 3.88130924, 4.2472914]\n",
    "# %timeit -r 15 got = get_false_rejections()\n",
    "got = get_false_rejections()\n",
    "assert np.allclose(expected, got), got\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 3\n",
    "\n",
    "Note this version does not properly integrate the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "p_threshold = np.repeat(0.4, d)\n",
    "\n",
    "\n",
    "def fast_invert(S, d):\n",
    "    for k in range(len(d)):\n",
    "        offset = (d[k] / (1 + d[k] * S[k, k])) * np.outer(\n",
    "            S[k],\n",
    "            S[..., k],\n",
    "        )  # I wonder how to cheaply represent this outer? but in C++ it should be just a trivial for-loop anyway\n",
    "        S = S - offset\n",
    "    return S\n",
    "\n",
    "\n",
    "# TODO: vectorize this in terms of sigma_sq\n",
    "# P(X | Y, theta)\n",
    "def conditional_exceed_prob_given_sigma(\n",
    "    sigma_sq: float,\n",
    "    mu_sig_sq: float,\n",
    "    sample_I,\n",
    "    thetahat,\n",
    "    thresh=logit(p_threshold),\n",
    "    mu_0=np.repeat(0, d),\n",
    "):\n",
    "    assert len(sigma_sq) == 1, sigma_sq\n",
    "    S_0 = np.diag(np.repeat(sigma_sq, d)) + mu_sig_sq\n",
    "    # V_0 = solve(S_0) #but because this is a known case of the form aI + bJ, we can use the explicit\n",
    "    # inverse formula, given by: 1/a I - J*(b/(a(a+db)))\n",
    "    V_0 = np.diag(np.repeat(1 / sigma_sq, d)) - (mu_sig_sq / sigma_sq) / (\n",
    "        sigma_sq + d * mu_sig_sq\n",
    "    )  # Note, by the way, that it's probably possible to use significant precomputation here\n",
    "    Sigma_posterior = fast_invert(S_0, sample_I)\n",
    "    # precision_posterior = V_0 + np.diag(sample_I)\n",
    "    # Sigma_posterior = np.linalg.inv(precision_posterior)\n",
    "    mu_posterior = Sigma_posterior @ (sample_I * thetahat + V_0 @ mu_0)\n",
    "\n",
    "    # What we now must return instead, is posterior threshold exceedance probabilities for each arm.\n",
    "    z_scores = (mu_posterior - thresh) / np.sqrt(np.diag(Sigma_posterior))\n",
    "    # return(list( thresh_exceed = thresh_exceed, mu_posterior = mu_posterior, Sigma_posterior = Sigma_posterior ))\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "# let's evaluate the endpoints of the prior in logspace-sigma:\n",
    "# determine endpoints:\n",
    "def get_false_rejections():\n",
    "\n",
    "    # Shared for a given prior\n",
    "    a = np.log(1e-8)\n",
    "    b = np.log(1e3)\n",
    "    n = 50\n",
    "    pts, weights = np.polynomial.legendre.leggauss(n)\n",
    "    pts = (pts + 1) * (b - a) / 2 + a\n",
    "    wts = weights * (b - a) / 2  # sum(wts) = b-a so it averages to 1 over space\n",
    "    density_logspace = scipy.stats.invgamma.pdf(\n",
    "        np.exp(pts), a=0.0005, scale=0.000005\n",
    "    ) * np.exp(pts)\n",
    "\n",
    "    # Shared for a given phat\n",
    "    phat = np.array([28, 14, 33, 36]) / 50\n",
    "    thetahat = logit(phat)\n",
    "    sample_I = n * phat * (1 - phat)  # diag(n*phat*(1-phat))\n",
    "\n",
    "    exceed_probs = np.apply_along_axis(\n",
    "        conditional_exceed_prob_given_sigma,\n",
    "        0,\n",
    "        np.exp(pts)[np.newaxis],\n",
    "        mu_sig_sq=0.1,  # TODO: integrate over this too\n",
    "        sample_I=sample_I,\n",
    "        thetahat=thetahat,\n",
    "        mu_0=np.repeat(0, d),\n",
    "    )\n",
    "\n",
    "    contributions = (\n",
    "        exceed_probs.T\n",
    "        * density_logspace[:, np.newaxis]\n",
    "        * wts[:, np.newaxis]\n",
    "        / np.sum(density_logspace * wts)\n",
    "    )  # this automatically fills in density_logspace by column\n",
    "    # Now, the final step is to sum\n",
    "    posterior_exceedance_prob = np.sum(contributions, 0)\n",
    "    return posterior_exceedance_prob\n",
    "\n",
    "\n",
    "expected = [3.18017917, 1.04572162, 3.88130924, 4.2472914]\n",
    "# %timeit -r 15 got = get_false_rejections()\n",
    "got = get_false_rejections()\n",
    "assert np.allclose(expected, got), got\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 4\n",
    "\n",
    "With proper hyperparameter integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def log_args(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f)\n",
    "        for arg in args:\n",
    "            print(arg)\n",
    "        for k,v in kwargs.items():\n",
    "            print(k, v)\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "def fast_invert(S, d):\n",
    "    for k in range(len(d)):\n",
    "        offset = (d[k] / (1 + d[k] * S[k, k])) * np.outer(\n",
    "            S[k],\n",
    "            S[..., k],\n",
    "        )  # I wonder how to cheaply represent this outer? but in C++ it should be just a trivial for-loop anyway\n",
    "        S = S - offset\n",
    "    return S\n",
    "\n",
    "\n",
    "# TODO: vectorize this in terms of sigma_sq\n",
    "# P(X | Y, theta)\n",
    "def conditional_exceed_prob_given_sigma(\n",
    "    sigma_sq: float,\n",
    "    mu_sig_sq: float,\n",
    "    sample_I,\n",
    "    thetahat,\n",
    "    thresh,\n",
    "    mu_0,\n",
    "):\n",
    "    assert len(sigma_sq) == 1, sigma_sq\n",
    "    S_0 = np.diag(np.repeat(sigma_sq, d)) + mu_sig_sq\n",
    "    # V_0 = solve(S_0) #but because this is a known case of the form aI + bJ, we can use the explicit\n",
    "    # inverse formula, given by: 1/a I - J*(b/(a(a+db)))\n",
    "    V_0 = np.diag(np.repeat(1 / sigma_sq, d)) - (mu_sig_sq / sigma_sq) / (\n",
    "        sigma_sq + d * mu_sig_sq\n",
    "    )  # Note, by the way, that it's probably possible to use significant precomputation here\n",
    "    Sigma_posterior = fast_invert(S_0, sample_I)\n",
    "    # precision_posterior = V_0 + np.diag(sample_I)\n",
    "    # Sigma_posterior = np.linalg.inv(precision_posterior)\n",
    "    mu_posterior = Sigma_posterior @ (sample_I * thetahat + V_0 @ mu_0)\n",
    "\n",
    "    # What we now must return instead, is posterior threshold exceedance probabilities for each arm.\n",
    "    z_scores = (mu_posterior - thresh) / np.sqrt(np.diag(Sigma_posterior))\n",
    "    quantiles = scipy.stats.norm.cdf(z_scores)\n",
    "    # return(list( thresh_exceed = thresh_exceed, mu_posterior = mu_posterior, Sigma_posterior = Sigma_posterior ))\n",
    "    return quantiles\n",
    "\n",
    "\n",
    "# let's evaluate the endpoints of the prior in logspace-sigma:\n",
    "# determine endpoints:\n",
    "def get_false_rejections():\n",
    "\n",
    "    # Shared for a given prior\n",
    "    a = np.log(1e-8)\n",
    "    b = np.log(1e3)\n",
    "    n_points = 50\n",
    "    mu_sig_sq = 100\n",
    "    mu_0 = -1.34\n",
    "    phat = np.array([3, 8, 5, 4]) / 15\n",
    "    d = 4\n",
    "    n = np.repeat(15, d)\n",
    "    p_threshold = np.repeat(0.3, d)\n",
    "    thetahat = logit(phat)\n",
    "    sample_I = n * phat * (1 - phat)  # diag(n*phat*(1-phat))\n",
    "    pts, weights = np.polynomial.legendre.leggauss(n_points)\n",
    "    pts = (pts + 1) * (b - a) / 2 + a\n",
    "    wts = weights * (b - a) / 2  # sum(wts) = b-a so it averages to 1 over space\n",
    "    density_logspace = scipy.stats.invgamma.pdf(\n",
    "        np.exp(pts), a=0.0005, scale=0.000005\n",
    "    ) * np.exp(pts)\n",
    "\n",
    "    density_sigma_given_y = [scipy.stats.multivariate_normal.pdf(\n",
    "        thetahat, np.repeat(mu_0, d), np.diag(sample_I**-1) + np.diag(np.repeat(sigma_sq, d)) + mu_sig_sq\n",
    "    ) for sigma_sq in np.exp(pts)]\n",
    "    # for pt in np.exp(pts):\n",
    "        # print(np.diag(sample_I**-1) + np.diag(np.repeat(pt, d)) + mu_sig_sq)\n",
    "    density_sigma_given_y = np.array(density_sigma_given_y)\n",
    "    print(density_sigma_given_y / density_sigma_given_y.sum())\n",
    "    final_weights = wts * density_logspace * density_sigma_given_y\n",
    "    final_weights /= np.sum(final_weights)\n",
    "\n",
    "    # Shared for a given phat\n",
    "\n",
    "    exceed_probs = np.apply_along_axis(\n",
    "        conditional_exceed_prob_given_sigma,\n",
    "        0,\n",
    "        np.exp(pts)[np.newaxis],\n",
    "        mu_sig_sq=mu_sig_sq,  # TODO: integrate over this too\n",
    "        sample_I=sample_I,\n",
    "        thetahat=thetahat,\n",
    "        mu_0=np.repeat(mu_0, d),\n",
    "        thresh=logit(p_threshold)\n",
    "    )\n",
    "    # print(exceed_probs.T)\n",
    "\n",
    "    contributions = (\n",
    "        exceed_probs.T * final_weights[:,np.newaxis]\n",
    "    )  # this automatically fills in density_logspace by column\n",
    "    # Now, the final step is to sum\n",
    "    posterior_exceedance_prob = np.sum(contributions, 0)\n",
    "    return posterior_exceedance_prob\n",
    "\n",
    "\n",
    "expected = [0.64462095, 0.80224266, 0.71778699, 0.67847136]\n",
    "# %timeit -r 15 got = get_false_rejections()\n",
    "got = get_false_rejections()\n",
    "assert np.allclose(expected, got), got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "n = 1000000\n",
    "x = np.linspace(0, 1, 1000000)\n",
    "y = scipy.stats.invgamma.logpdf(x, a=0.0005, scale=0.000005)\n",
    "plt.plot(x, y)\n",
    "# plt.xlim(0, .01)\n",
    "plt.title(\"InvGamma(0.0005, 0.000005) PDF\")\n",
    "print(np.sum(y[int(n*.01):]) /n )\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0005\n",
    "beta = 0.000005\n",
    "invgamma = scipy.stats.invgamma\n",
    "# x = np.linspace(invgamma.ppf(0.001, alpha, scale=beta), invgamma.ppf(0.999, alpha, scale=beta), 100)\n",
    "# x = np.linspace(0, 1.0, 1000000)\n",
    "plt.plot(x, invgamma.pdf(x, alpha, scale=beta), \"r-\", label=\"invgamma pdf\")\n",
    "plt.ylabel('density')\n",
    "plt.xlabel('$\\sigma^2$')\n",
    "plt.title('InverseGamma(0.0005, 0.000005)')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.sqrt(x), invgamma.logpdf(x, alpha, scale=beta), \"r-\", label=\"invgamma pdf\")\n",
    "plt.ylabel('log density')\n",
    "plt.xlabel('$\\sigma$')\n",
    "plt.title('InverseGamma(0.0005, 0.000005)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def logit_normal_pdf(x, loc, scale):\n",
    "    return (\n",
    "        np.sqrt(scale * 2 * np.pi) ** -1\n",
    "        * (x * (1 - x)) ** -1\n",
    "        * np.exp(-((logit(x) - loc) ** 2) / (2 * scale))\n",
    "    )\n",
    "\n",
    "\n",
    "n = 50\n",
    "a = 3\n",
    "b = 3\n",
    "aobserved = 1\n",
    "\n",
    "def compare_dirty(n, a, b, p):\n",
    "    aobserved = n*p\n",
    "    approximation = dirtymultilevel(\n",
    "        n=np.repeat(n, d),\n",
    "        phat=np.array([aobserved]) / n,\n",
    "        mu0=np.repeat(0, d),\n",
    "        V0=np.diag(np.repeat(1, d)),\n",
    "    )\n",
    "    mu = approximation[\"mu_posterior\"][0]\n",
    "    sigma = approximation[\"Sigma_posterior\"][0][0]\n",
    "    x = np.arange(0, 1, 0.005)\n",
    "    y = scipy.stats.beta(a=a + aobserved, b=b + n - aobserved).pdf(x)\n",
    "    y2 = logit_normal_pdf(x, mu, sigma)\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.plot(x, y, label=f\"binomial({a+aobserved},{b+n-aobserved})\")\n",
    "    plt.plot(x, y2, label=f\"logit-normal({mu:.2f},{sigma:.2f})\")\n",
    "    plt.title(\"PDFs\")\n",
    "    plt.xlim(0, 0.5)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "w = widgets.interactive(compare_dirty, n=(5,100), a=(0,3), b=(0,3), p=(0,1, 0.1))\n",
    "# from IPython.display import display\n",
    "# display(w)\n",
    "output = w.children[-1]\n",
    "output.layout.height = '550px'\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestDirtyMultiLevel(unittest.TestCase):\n",
    "    def test(self):\n",
    "        want = dict(\n",
    "            mu_posterior=np.array([0.2230568, -0.85922139, 0.60901482, 0.85922139]),\n",
    "            Sigma_posterior=np.array(\n",
    "                [\n",
    "                    [0.07507508, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.09025271, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.08183306, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.09025271],\n",
    "                ]\n",
    "            ),\n",
    "            conf_logit=np.array(\n",
    "                [\n",
    "                    [-0.31397989, 0.2230568, 0.76009349],\n",
    "                    [-1.44804632, -0.85922139, -0.27039646],\n",
    "                    [0.04832785, 0.60901482, 1.1697018],\n",
    "                    [0.27039646, 0.85922139, 1.44804632],\n",
    "                ]\n",
    "            ),\n",
    "            conf_prob=np.array(\n",
    "                [\n",
    "                    [0.42214359, 0.55553413, 0.68137403],\n",
    "                    [0.19030242, 0.29750204, 0.43280977],\n",
    "                    [0.51207961, 0.64771604, 0.76309111],\n",
    "                    [0.56719023, 0.70249796, 0.80969758],\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # This one is intended to match Ben's example!\n",
    "        got = dirtymultilevel(\n",
    "            n=np.repeat(50, d),\n",
    "            phat=np.array([28, 14, 33, 36]) / 50,\n",
    "            mu0=np.repeat(0, d),\n",
    "            V0=np.diag(np.repeat(1, d)),\n",
    "            return_ci=True,\n",
    "        )\n",
    "\n",
    "        for key in want:\n",
    "            self.assertTrue(\n",
    "                np.allclose(want[key], got[key]),\n",
    "                \"dictionary values should be almost equal\",\n",
    "            )\n",
    "\n",
    "\n",
    "unittest.main(argv=[\"first-arg-is-ignored\"], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6003881daa8d5609c65594d703c96226156610b5a8c372e4183621449f43a4fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('trading-GoF_m6Ee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
